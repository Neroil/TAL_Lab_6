{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\"/>\n",
    "\n",
    "# Cours TAL - Laboratoire 6\n",
    "# Trois méthodes de désambiguïsation lexicale\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "L'objectif de ce laboratoire est d'implémenter et de comparer plusieurs méthodes de désambiguïsation lexicale (en anglais, *Word Sense Disambiguation* ou WSD).  Vous utiliserez un corpus avec plusieurs milliers de phrases, chaque phrase contenant une occurrence du mot anglais *interest* annotée avec le sens que ce mot possède dans la phrase respective.  Les trois méthodes sont les suivantes (elles seront détaillées par la suite) :\n",
    "\n",
    "* Algorithme de Lesk simplifié.\n",
    "* Utilisation de word2vec.\n",
    "* Classification supervisée utilisant des traits lexicaux.\n",
    "\n",
    "Les deux premières méthodes n'utilisent pas l'apprentissage automatique.  Elles fonctionnent selon le même principe : comparer le contexte d'une occurrence de *interest* avec chacune des définitions des sens (*synsets*) et choisir la définition la plus proche du contexte.  L'algorithme de Lesk définit la proximité comme le nombre de mots en commun, alors que word2vec la calcule comme la similarité de vecteurs.  La dernière méthode vise à classifier les occurrences de *interest*, les sens étant les classes, et les attributs étant les mots du contexte (apprentissage supervisé)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyse des données\n",
    "\n",
    "Téléchargez le corpus *interest* depuis le [site du Prof. Ted Pedersen](http://www.d.umn.edu/~tpederse/data.html) (il se trouve en bas de sa page web).  Téléchargez l'archive ZIP marquée *original format without POS tags* et extrayez le fichier `interest-original.txt`.  Téléchargez également le fichier `README.int.txt` indiqué à la ligne au-dessus. Veuillez répondre brièvement aux questions suivantes :\n",
    "\n",
    "a. Quelles sont les URL du fichier ZIP et celle du fichier `README.int.txt` ?\n",
    "\n",
    "b. Quel est le format du fichier `interest-original.txt` et comment sont annotés les sens de *interest* ?\n",
    "\n",
    "c. Est-ce qu'il y a aussi des occurrences au pluriel (*interests*) à traite ?\n",
    "\n",
    "d. Comment sont annotées les phrases qui contiennent plusieurs occurrences du mot *interest* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Quelles sont les URL du fichier ZIP et celle du fichier `README.int.txt` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZIP: https://www.d.umn.edu/~tpederse/Data/interest-original.nopos.tar.gz\n",
    "# README.int.txt: https://www.d.umn.edu/~tpederse/Data/README.int.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** Quel est le format du fichier `interest-original.txt` et comment sont annotés les sens de *interest* ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le fichier est un corpus composé de 2369 phrases extraites du Wall Street Journal (Penn Treebank).\n",
    "# Chaque phrase est séparée par une ligne contenant uniquement $$.\n",
    "# Chaque phrase contient exactement une occurrence annotée du mot interest ou interests (au pluriel).\n",
    "# Les sens sont annotés manuellement selon 6 sens définis par le Longman Dictionary of Contemporary English (LDOCE) :\n",
    "# interest_1 : attention ou curiosité\n",
    "# interest_2 : qualité de susciter l'attention\n",
    "# interest_3 : activité à laquelle on porte attention\n",
    "# interest_4 : avantage ou intérêt personnel\n",
    "# interest_5 : part d'une entreprise ou affaire\n",
    "# interest_6 : intérêt au sens financier (argent)\n",
    "# L’annotation suit ce format : interest_6/NN (mot + numéro de sens + étiquette morpho-syntaxique POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1c.** Est-ce qu'il y a aussi des occurrences au pluriel (*interests*) à traite ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oui. Le corpus contient des occurrences au pluriel annotées de la même façon : interests_5/NNS, interests_4/NNS, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1d.** Comment sont annotées les phrases qui contiennent plusieurs occurrences du mot *interest* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaque occurrence unique dans une phrase est annotée individuellement avec son propre numéro de sens (interest_1, interest_5, etc.).\n",
    "# Ces phrases restent dans une seule ligne séparée par $$ comme les autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1e.** D'après le fichier `README.int.txt`, quelles sont les définitions des six sens de *interest* annotés dans les données et quelles sont leurs fréquences ? Vous pouvez copier/coller l'extrait de `README`ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici (en commentaire) à la question.\n",
    "# Sense 1 =  361 occurrences (15%) - readiness to give attention  \n",
    "# Sense 2 =   11 occurrences (01%) - quality of causing attention to be given to  \n",
    "# Sense 3 =   66 occurrences (03%) - activity, etc. that one gives attention to  \n",
    "# Sense 4 =  178 occurrences (08%) - advantage, advancement or favor  \n",
    "# Sense 5 =  500 occurrences (21%) - a share in a company or business  \n",
    "# Sense 6 = 1252 occurrences (53%) - money paid for the use of money  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1f.** De quel dictionnaire viennent les sens précédents ? Où peut-on le consulter en ligne ?  Veuillez aligner les définitions du dictionnaire avec les six sens annotés en écrivant par exemple `Sense 3 = \"an activity that you enjoy doing or a subject that you enjoy studying\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici (en commentaire) à la question.\n",
    "# Les sens viennent du Longman Dictionary of Contemporary English (LDOCE).\n",
    "# Il est mentionné dans le README : « the six non-idiomatic noun senses of \"interest\" defined in the electronic version of the first edition of Longman's Dictionary of Contemporary English ».\n",
    "# On peut consulter une version en ligne ici : https://www.ldoceonline.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignement des définitions du dictionnaire (exemples reformatés) :\n",
    "#    Sense 1 = \"readiness to give attention\"\n",
    "#        e.g., \"He listened with great interest.\"\n",
    "#    Sense 2 = \"quality of causing attention to be given to\"\n",
    "#        e.g., \"The story is of little interest.\"\n",
    "#    Sense 3 = \"an activity that you enjoy doing or subject that you enjoy studying\"\n",
    "#        e.g., \"My main interests are photography and hiking.\"\n",
    "#    Sense 4 = \"advantage, benefit or self-serving motive\"\n",
    "#        e.g., \"She acted in her own best interest.\"\n",
    "#    Sense 5 = \"a share in a business or financial involvement\"\n",
    "#        e.g., \"He bought a controlling interest in the company.\"\n",
    "#    Sense 6 = \"money paid for the use of money\"\n",
    "#        e.g., \"The bank charges 5% interest.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1g.** En consultant [WordNet en ligne](http://wordnetweb.princeton.edu/perl/webwn), trouvez les définitions des synsets  pour le **nom commun** *interest*.  Combien de synsets y a-t-il ?  Veuillez indiquer comme avant la **définition** de chaque synset pour chacun des six sens ci-dessus (au besoin, fusionner ou ignorer des synsets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/massimo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de synsets pour 'interest' (nom) : 7\n",
      "Synset 1 : interest.n.01\n",
      "Définition : a sense of concern with and curiosity about someone or something\n",
      "Lemmes : ['interest', 'involvement']\n",
      "Exemples : ['an interest in music']\n",
      "=================================\n",
      "Synset 2 : sake.n.01\n",
      "Définition : a reason for wanting something done\n",
      "Lemmes : ['sake', 'interest']\n",
      "Exemples : ['for your sake', 'died for the sake of his country', 'in the interest of safety', 'in the common interest']\n",
      "=================================\n",
      "Synset 3 : interest.n.03\n",
      "Définition : the power of attracting or holding one's attention (because it is unusual or exciting etc.)\n",
      "Lemmes : ['interest', 'interestingness']\n",
      "Exemples : ['they said nothing of great interest', 'primary colors can add interest to a room']\n",
      "=================================\n",
      "Synset 4 : interest.n.04\n",
      "Définition : a fixed charge for borrowing money; usually a percentage of the amount borrowed\n",
      "Lemmes : ['interest']\n",
      "Exemples : ['how much interest do you pay on your mortgage?']\n",
      "=================================\n",
      "Synset 5 : interest.n.05\n",
      "Définition : (law) a right or legal share of something; a financial involvement with something\n",
      "Lemmes : ['interest', 'stake']\n",
      "Exemples : ['they have interests all over the world', \"a stake in the company's future\"]\n",
      "=================================\n",
      "Synset 6 : interest.n.06\n",
      "Définition : (usually plural) a social group whose members control some field of activity and who have common aims\n",
      "Lemmes : ['interest', 'interest_group']\n",
      "Exemples : ['the iron interests stepped up production']\n",
      "=================================\n",
      "Synset 7 : pastime.n.01\n",
      "Définition : a diversion that occupies one's time and thoughts (usually pleasantly)\n",
      "Lemmes : ['pastime', 'interest', 'pursuit']\n",
      "Exemples : ['sailing is her favorite pastime', 'his main pastime is gambling', 'he counts reading among his interests', 'they criticized the boy for his limited pursuits']\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "synsets = wn.synsets(\"interest\", pos=wn.NOUN)\n",
    "print(f\"Nombre de synsets pour 'interest' (nom) : {len(synsets)}\")\n",
    "for i, syn in enumerate(synsets):\n",
    "    print(f\"Synset {i+1} : {syn.name()}\")\n",
    "    print(f\"Définition : {syn.definition()}\")\n",
    "    print(f\"Lemmes : {syn.lemma_names()}\")\n",
    "    print(f\"Exemples : {syn.examples()}\")\n",
    "    print(f\"=================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sense 1** = \"readiness to give attention\"  \n",
    "  → WordNet : *interest.n.01*  \n",
    "  → Définition : *a sense of concern with and curiosity about someone or something*\n",
    "\n",
    "- **Sense 2** = \"quality of causing attention to be given to\"  \n",
    "  → WordNet : *interest.n.03*  \n",
    "  → Définition : *the power of attracting or holding one's attention (because it is unusual or exciting etc.)*\n",
    "\n",
    "- **Sense 3** = \"activity, etc. that one gives attention to\"  \n",
    "  → WordNet : *interest.n.07*  \n",
    "  → Définition : *a diversion that occupies one's time and thoughts (usually pleasantly)*\n",
    "\n",
    "- **Sense 4** = \"advantage, advancement or favor\"  \n",
    "  → WordNet : *sake.n.01*  \n",
    "  → Définition : *a reason for wanting something done*\n",
    "\n",
    "- **Sense 5** = \"a share in a company or business\"  \n",
    "  → WordNet : *interest.n.05*  \n",
    "  → Définition : *(law) a right or legal share of something; a financial involvement with something*\n",
    "\n",
    "- **Sense 6** = \"money paid for the use of money\"  \n",
    "  → WordNet : *interest.n.04*  \n",
    "  → Définition : *a fixed charge for borrowing money; usually a percentage of the amount borrowed*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1h.** Définissez (manuellement, ou avec quelques lignes de code) une liste nommée `senses1` avec les mots des définitions du README, en supprimant les stopwords (p.ex. les mots < 4 lettres).  Affichez la liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['readiness', 'attention', 'give'], ['given', 'attention', 'causing', 'quality'], ['attention', 'activity', 'gives'], ['advantage', 'advancement', 'favor'], ['business', 'company', 'share'], ['money', 'paid']]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question et créer la variable 'senses1' (liste de 6 listes de chaînes).\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "definitions = [\n",
    "    \"readiness to give attention\",\n",
    "    \"quality of causing attention to be given to\",\n",
    "    \"activity, etc. that one gives attention to\",\n",
    "    \"advantage, advancement or favor\",\n",
    "    \"a share in a company or business\",\n",
    "    \"money paid for the use of money\"\n",
    "]\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) | set(string.punctuation)\n",
    "senses1 = []\n",
    "for definition in definitions:\n",
    "    tokens = word_tokenize(definition.lower())\n",
    "    filtered = [w for w in tokens if w.isalpha() and len(w) >= 4 and w not in stop_words]\n",
    "    unique_words = list(set(filtered))\n",
    "    senses1.append(unique_words)\n",
    "\n",
    "print(senses1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1i.** En combinant les définitions obtenues aux points (4) et (5) ci-dessus, construisez une liste nommée `senses2` avec pour chacun des sens de *interest* une liste de **mots-clés** correspondants.  Vous pouvez concaténer les définitions, puis écrire des instructions en Python pour extraire les mots (uniques).  Respectez l'ordre des sens données par `README`, et à la fin affichez `senses2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['attention', 'curiosity', 'concern', 'readiness'], ['attention', 'attracting', 'excit£ing', 'noteworthy', 'unusual'], ['hobby', 'activity', 'diversion', 'recreation', 'pastime'], ['benefit', 'advantage', 'favor', 'gain', 'profit'], ['share', 'ownership', 'stake', 'equity', 'business', 'company'], ['money', 'loan', 'borrowing', 'charge', 'percentage', 'payment']]\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question et créer la variable 'senses2' (liste de 6 listes de chaînes).\n",
    "senses2 = [\n",
    "    [\"attention\", \"curiosity\", \"concern\", \"readiness\"],\n",
    "    [\"attention\", \"attracting\", \"excit£ing\", \"noteworthy\", \"unusual\"],\n",
    "    [\"hobby\", \"activity\", \"diversion\", \"recreation\", \"pastime\"],\n",
    "    [\"benefit\", \"advantage\", \"favor\", \"gain\", \"profit\"],\n",
    "    [\"share\", \"ownership\", \"stake\", \"equity\", \"business\", \"company\"],\n",
    "    [\"money\", \"loan\", \"borrowing\", \"charge\", \"percentage\", \"payment\"]\n",
    "]\n",
    "\n",
    "print(senses2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1j.** Chargez les données depuis `interest-original.txt` dans une liste appelée `sentences` qui contient pour chaque phrase la liste des mots (sans les séparateurs *$$* et *===...*).  Ces phrases sont-elles déjà tokenisées en mots ?  Sinon, faites-le.  À ce stade, ne modifiez pas encore les occurrences annotées *interest(s)\\_X*.  Comptez le nombre total de phrases et affichez-en trois au hasard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 2368 phrases.\n",
      "En voici 3 au hasard :\n",
      "but he said yesterday that the court had gone `` into the matter with great care and the public interest_4 has been served '' by the sentence .\n",
      "====================================== currently , mr. merksamer owns 20 % of the company ; l.j. hooker acquired its 80 % interest_5 in the firm in may 1986 .\n",
      "the zero-coupon subordinated notes have no periodic interest_6 payments .\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "with open(\"interest-original.txt\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "sentences = [s.strip().split() for s in data.split(\"$$\") if s.strip()]\n",
    "\n",
    "print(\"Il y a {} phrases.\\nEn voici 3 au hasard :\".format(len(sentences)))\n",
    "from random import sample\n",
    "for s in sample(sentences, 3):\n",
    "    print(\" \".join(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithme de Lesk simplifié\n",
    "\n",
    "**2a.** Définissez une fonction `wsd_lesk(senses, sentence)` qui prend deux arguments : une liste de listes de mots-clés (comme `senses1` et `senses2` ci-dessus) et une phrase avec une occurrence annotée de *interest* ou *interests*, et qui retourne l'index du sens le plus probable (entre 1 et 6) selon l'algorithme de Lesk.  Cet algorithme choisit le sens qui a le maximum de mots en commun avec le contexte de *interest*.  Vous pouvez choisir vous-mêmes la taille de ce voisinage (`window_size`).  En cas d'égalité entre deux sens, tirer la réponse au sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "\n",
    "def wsd_lesk(senses, sentence, window_size=8):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Trouver l'index de \"interest\" annoté avec un sens ou \"interests\"\n",
    "    index_interest = next(i for i, word in enumerate(sentence) if (word.startswith(\"interest_\") or word.startswith(\"interests_\")))\n",
    "    \n",
    "    # Nettoyage du contexte (voisinage gauche + droite)\n",
    "    context = sentence[max(0, index_interest - window_size): index_interest] + \\\n",
    "              sentence[index_interest + 1: index_interest + 1 + window_size]\n",
    "    \n",
    "    context_clean = [w.lower() for w in context if w.lower() not in stop_words and len(w) > 3]\n",
    "    \n",
    "    # Compter le chevauchement avec chaque sens\n",
    "    overlaps = []\n",
    "    for sense_keywords in senses:\n",
    "        overlap = len(set(context_clean) & set(sense_keywords))\n",
    "        overlaps.append(overlap)\n",
    "    \n",
    "    # Résoudre égalités au hasard\n",
    "    max_overlap = max(overlaps)\n",
    "    candidates = [i for i, v in enumerate(overlaps) if v == max_overlap]\n",
    "    \n",
    "    return random.choice(candidates) + 1  # retour 1 à 6 (pas 0 à 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Définissez maintenant une fonction `evaluate_wsd(fct_name, senses, sentences)` qui prend en paramètre le nom de la méthode de similarité (pour commencer : `wsd_lesk`) ainsi que la liste des mots-clés par sens, et la liste de phrases, et qui retourne le score de la méthode de similarité.  Ce score sera tout simplement le pourcentage de réponses correctes (sens trouvé identique au sens annoté)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "def evaluate_wsd(fct_name, senses, sentences, window_size=8):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Extraire le vrai sens\n",
    "        target = next((w for w in sentence if (w.startswith(\"interest_\") or w.startswith(\"interests_\")) ), None)\n",
    "        if not target:\n",
    "            continue\n",
    "        true_sense = int(target.split(\"_\")[1].split(\"/\")[0])\n",
    "        \n",
    "        predicted_sense = fct_name(senses, sentence, window_size)\n",
    "        \n",
    "        if predicted_sense == true_sense:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** En fixant au mieux la taille de la fenêtre autour de *interest*, quel est le meilleur score de la méthode de Lesk simplifiée ?  Quelle liste de sens conduit à de meilleurs scores, `senses1` ou `senses2` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Évaluation avec senses1 :\n",
      "fenêtre 1 → 16.26%\n",
      "fenêtre 2 → 16.09%\n",
      "fenêtre 3 → 17.53%\n",
      "fenêtre 4 → 17.78%\n",
      "fenêtre 5 → 19.93%\n",
      "fenêtre 6 → 19.72%\n",
      "fenêtre 7 → 18.83%\n",
      "fenêtre 8 → 19.13%\n",
      "fenêtre 9 → 20.40%\n",
      "fenêtre 10 → 20.65%\n",
      "fenêtre 11 → 20.95%\n",
      "fenêtre 12 → 21.16%\n",
      "fenêtre 13 → 20.61%\n",
      "fenêtre 14 → 19.38%\n",
      "fenêtre 15 → 20.73%\n",
      "fenêtre 16 → 21.58%\n",
      "fenêtre 17 → 20.40%\n",
      "fenêtre 18 → 21.54%\n",
      "fenêtre 19 → 20.57%\n",
      "fenêtre 20 → 22.09%\n",
      "\n",
      "Évaluation avec senses2 :\n",
      "fenêtre 1 → 18.58%\n",
      "fenêtre 2 → 18.96%\n",
      "fenêtre 3 → 19.59%\n",
      "fenêtre 4 → 19.64%\n",
      "fenêtre 5 → 21.16%\n",
      "fenêtre 6 → 22.26%\n",
      "fenêtre 7 → 22.59%\n",
      "fenêtre 8 → 23.02%\n",
      "fenêtre 9 → 23.82%\n",
      "fenêtre 10 → 23.06%\n",
      "fenêtre 11 → 23.35%\n",
      "fenêtre 12 → 23.69%\n",
      "fenêtre 13 → 24.11%\n",
      "fenêtre 14 → 23.10%\n",
      "fenêtre 15 → 23.02%\n",
      "fenêtre 16 → 23.52%\n",
      "fenêtre 17 → 24.07%\n",
      "fenêtre 18 → 24.07%\n",
      "fenêtre 19 → 24.49%\n",
      "fenêtre 20 → 24.83%\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "\n",
    "senses = [senses1, senses2]\n",
    "\n",
    "# Veuillez répondre ici à la question.\n",
    "sens = [senses1, senses2]\n",
    "for i, s in enumerate(sens, 0):\n",
    "    print(f\"\\nÉvaluation avec senses{i+1} :\")\n",
    "    for ws in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20):\n",
    "        score = evaluate_wsd(\n",
    "            lambda sens, sent, _: wsd_lesk(sens, sent, window_size=ws),\n",
    "            sens[i], sentences)\n",
    "        print(f\"fenêtre {ws} → {score:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilisation de word2vec pour la similarité contexte vs. synset\n",
    "\n",
    "**3a.** En réutilisant une partie du code de `wsd_lesk`, veuillez maintenant définir une fonction `wsd_word2vec(senses, sentence)` qui choisit le sens en utilisant la similarité **word2vec** étudiée dans le labo précédent. \n",
    "* Vous pouvez chercher dans la [documentation des KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html) comment calculer directement la similarité entre deux listes de mots.\n",
    "* Comme `wsd_lesk`, la nouvelle fonction `wsd_word2vec` prend en argument une liste de listes de mots-clés par sens (comme `senses1` et `senses2` ci-dessus), et une phrase avec une occurrence annotée de *interest* ou *interests*.\n",
    "* La fonction retourne le numéro du sens le plus probable selon la similarité word2vec entre les mots du sens et ceux du voisinage de *interest*.  En cas d'égalité, tirer le sens au sort.\n",
    "* Vous pouvez régler la taille du voisinage (`window_size`) par l'expérimentation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader as api\n",
    "w2v_vectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "path_to_model = \"~/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\" # à adapter\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format(path_to_model, binary=True)  # C bin format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "\n",
    "def wsd_word2vec(senses, sentence, window_size=8,\n",
    "                 target_prefixes=(\"interest_\", \"interests_\")):\n",
    "    \"\"\"\n",
    "    Choisit le sens d’une occurrence de « interest / interests » par similarité Word2Vec.\n",
    "    Retourne un entier entre 1 et len(senses).\n",
    "    Compatible avec evaluate_wsd(senses, sentence, window_size=...).\n",
    "    \"\"\"\n",
    "    # 1. index du mot annoté\n",
    "    idx = next(i for i, w in enumerate(sentence)\n",
    "               if any(w.startswith(pref) for pref in target_prefixes))\n",
    "\n",
    "    # 2. contexte (on enlève le mot cible)\n",
    "    left  = sentence[max(0, idx - window_size): idx]\n",
    "    right = sentence[idx + 1: idx + 1 + window_size]\n",
    "    raw_context = left + right\n",
    "\n",
    "    # 3. nettoyage (stop-words)\n",
    "    sw = set(stopwords.words(\"english\"))\n",
    "    context = [w.lower() for w in raw_context\n",
    "               if w.lower() not in sw and len(w) > 3 and w in wv_model]\n",
    "\n",
    "    if not context: \n",
    "        # fallback aléatoire pour si le contexte est vide après nettoyage des stop-words\n",
    "        return random.randint(1, len(senses))\n",
    "\n",
    "    # 4. similarité contexte ↔ signature pour chaque sens\n",
    "    sims = []\n",
    "    for sig in senses:\n",
    "        sig = [w for w in sig if w in wv_model]\n",
    "        sims.append(\n",
    "            wv_model.n_similarity(context, sig) if sig else -1)\n",
    "\n",
    "    # 5. meilleur score (tirage en cas d’égalité)\n",
    "    best = [i for i, s in enumerate(sims) if s == max(sims)]\n",
    "    return random.choice(best) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Appliquez maintenant la même méthode `evaluate_wsd` avec la fonction `wsd_word2vec` (en cherchant une bonne valeur de la taille de la fenêtre) et affichez le score de la similarité word2vec.  Comment se compare-t-il avec le score précédent (Lesk) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Évaluation avec senses1 :\n",
      "fenêtre 1 → 24.54%\n",
      "fenêtre 2 → 29.39%\n",
      "fenêtre 3 → 32.94%\n",
      "fenêtre 4 → 37.04%\n",
      "fenêtre 5 → 37.33%\n",
      "fenêtre 6 → 37.75%\n",
      "fenêtre 7 → 38.39%\n",
      "fenêtre 8 → 38.68%\n",
      "fenêtre 9 → 39.61%\n",
      "fenêtre 10 → 40.03%\n",
      "fenêtre 11 → 41.05%\n",
      "fenêtre 12 → 41.43%\n",
      "fenêtre 13 → 40.75%\n",
      "fenêtre 14 → 40.62%\n",
      "fenêtre 15 → 40.20%\n",
      "fenêtre 16 → 40.50%\n",
      "fenêtre 17 → 40.62%\n",
      "fenêtre 18 → 40.54%\n",
      "fenêtre 19 → 40.41%\n",
      "fenêtre 20 → 40.46%\n",
      "\n",
      "Évaluation avec senses2 :\n",
      "fenêtre 1 → 55.91%\n",
      "fenêtre 2 → 57.31%\n",
      "fenêtre 3 → 58.36%\n",
      "fenêtre 4 → 59.76%\n",
      "fenêtre 5 → 61.23%\n",
      "fenêtre 6 → 61.66%\n",
      "fenêtre 7 → 61.78%\n",
      "fenêtre 8 → 61.57%\n",
      "fenêtre 9 → 61.70%\n",
      "fenêtre 10 → 61.70%\n",
      "fenêtre 11 → 62.20%\n",
      "fenêtre 12 → 62.29%\n",
      "fenêtre 13 → 62.42%\n",
      "fenêtre 14 → 62.25%\n",
      "fenêtre 15 → 62.04%\n",
      "fenêtre 16 → 62.16%\n",
      "fenêtre 17 → 61.70%\n",
      "fenêtre 18 → 62.04%\n",
      "fenêtre 19 → 62.12%\n",
      "fenêtre 20 → 62.25%\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "sens = [senses1, senses2]\n",
    "for i, s in enumerate(sens, 0):\n",
    "    print(f\"\\nÉvaluation avec senses{i+1} :\")\n",
    "    for ws in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20):\n",
    "        score = evaluate_wsd(\n",
    "            lambda sens, sent, _: wsd_word2vec(sens, sent, window_size=ws),\n",
    "            sens[i], sentences)\n",
    "        print(f\"fenêtre {ws} → {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification supervisée avec des traits lexicaux\n",
    "Vous entraînerez maintenant des classifieurs pour prédire le sens d'une occurrence dans une phrase.  Le premier but sera de transformer chaque phrase en un ensemble d'attributs pour formater les données en vue des expériences de classification.\n",
    "\n",
    "Veuillez utiliser le classifieur `NaiveBayesClassifier` fourni par NLTK.  Le mode d'emploi se trouve dans le [Chapitre 6, sections 1.1-1.3](https://www.nltk.org/book/ch06.html) du livre NLTK.  Consultez-le attentivement pour trouver comment formater les données.  De plus, il faudra séparer les données en sous-ensembles d'entraînement et de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vous propose de nommer les attributs `word-k`, ..., `word-2`, `word-1`, `word+1`, `word+2`, ..., `word+k` (fenêtre de taille `2*k` autour de *interest*).  Leurs valeurs sont les mots observés aux emplacements respectifs, ou `NONE` si la position dépasse l'étendue de la phrase.  Vous ajouterez un attribut nommé `word0` qui est l'occurrence du mot *interest* au singulier ou au pluriel.  \n",
    "\n",
    "Pour chaque occurrence de *interest*, vous devrez donc créer la représentation suivante (où `6` est le numéro du sens, essentiel pour l'entraînement, mais à cacher lors de l'évaluation) :\n",
    "```\n",
    "[{'word-1': 'in', 'word+1': 'rates', 'word-2': 'declines', 'word+2': 'NONE', 'word0': 'interest'}, 6]\n",
    "```\n",
    "\n",
    "**4a.** En partant de la liste des phrases appelée `sentences` préparée plus haut, veuillez générer la liste avec toutes les représentation, appelée `items_with_features`.  Vous pouvez vous aider du livre NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'instances: 2368\n",
      "Exemples d'instances:\n",
      "[{'word0': 'interest', 'word-1': 'in', 'word+1': 'rates', 'word-2': 'declines', 'word+2': '.'}, '6']\n",
      "[{'word0': 'interest', 'word-1': 'declining', 'word+1': 'rates', 'word-2': 'indicate', 'word+2': 'because'}, '6']\n",
      "[{'word0': 'interest', 'word-1': 'short-term', 'word+1': 'rates', 'word-2': 'in', 'word+2': '.'}, '6']\n"
     ]
    }
   ],
   "source": [
    "# Veuillez répondre ici à la question.\n",
    "items_with_features = []\n",
    "\n",
    "\n",
    "def extract_features(input_sentences, k):\n",
    "    result_features = []\n",
    "\n",
    "    for sentence in input_sentences:\n",
    "        \n",
    "        for i, word in enumerate(sentence):\n",
    "            if word.startswith('interest'):\n",
    "                features = {}\n",
    "                sens = \"\"\n",
    "                \n",
    "                try:\n",
    "                    if '_' in word: # We got a match !\n",
    "                        (word0, sens) = word.split('_')\n",
    "                        features['word0'] = word0\n",
    "                        \n",
    "                        for depth in range(1, k + 1):\n",
    "                            # Left context\n",
    "                            if i - depth >= 0:\n",
    "                                features[f'word-{depth}'] = sentence[i - depth].lower()\n",
    "                            else:\n",
    "                                features[f'word-{depth}'] = 'NONE'\n",
    "                            \n",
    "                            # Right context\n",
    "                            if i + depth < len(sentence):\n",
    "                                features[f'word+{depth}'] = sentence[i + depth].lower()\n",
    "                            else:\n",
    "                                features[f'word+{depth}'] = 'NONE'\n",
    "                            \n",
    "        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing word '{word}' in {sentence}: {e}\")\n",
    "                    \n",
    "                result_features.append([features, sens])\n",
    "                break\n",
    "            \n",
    "    return result_features\n",
    "                \n",
    "items_with_features = extract_features(sentences, 2)\n",
    "\n",
    "print(f\"Nombre total d'instances: {len(items_with_features)}\")\n",
    "print(\"Exemples d'instances:\")\n",
    "for i in range(min(3, len(items_with_features))):\n",
    "    print(items_with_features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Veuillez séparer les données aléatoirement en 80% pour l'entraînement et 20%  pour l'évaluation.  Veuillez faire une division stratifiée : les deux sous-ensembles doivent contenir les mêmes proportions de sens que l'ensemble de départ.  Ils seront appelés `iwf_train` et `iwf_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data, new length: 2367\n",
      "[({'word0': 'interest', 'word-1': 'short', 'word+1': 'dropped', 'word-2': 'that', 'word+2': 'to'}, '5'), ({'word0': 'interest', 'word-1': '}', 'word+1': 'rates', 'word-2': 'NONE', 'word+2': ','}, '6'), ({'word0': 'interests', 'word-1': 'minority', 'word+1': 'rose', 'word-2': 'to', 'word+2': 'to'}, '5')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data_train_test(items_with_features, test_size=0.2):\n",
    "\n",
    "    # Cleanup empty features or senses\n",
    "    items_with_features = [item for item in items_with_features if item[1] != '']\n",
    "    print(f\"Cleaned data, new length: {len(items_with_features)}\")\n",
    "\n",
    "\n",
    "    X = [item[0] for item in items_with_features]\n",
    "    y = [item[1] for item in items_with_features]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    return list(zip(X_train, y_train)), list(zip(X_test, y_test))\n",
    "\n",
    "iwf_train, iwf_test = split_data_train_test(items_with_features, test_size=0.2)\n",
    "\n",
    "\n",
    "print(iwf_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total : 2368\n",
      "Train : 1893 | Test : 474\n",
      "Sample train: ({'word0': 'interest', 'word-1': 'in', 'word+1': 'rates', 'word-2': 'dips', 'word+2': '.'}, '6')\n",
      "Sample test : ({'word0': 'interest', 'word-1': 'short', 'word+1': 'dropped', 'word-2': 'that', 'word+2': 'to'}, '5')\n"
     ]
    }
   ],
   "source": [
    "print(\"Total :\", len(items_with_features))\n",
    "print(\"Train :\", len(iwf_train), \"| Test :\", len(iwf_test))\n",
    "print(\"Sample train:\", iwf_train[0])\n",
    "print(\"Sample test :\", iwf_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Veuillez créer une instance de `NaiveBayesClassifier`, l'entraîner sur `iwf_train` et la tester sur `iwf_test` (voir la documentation NLTK).  En expérimentant avec différentes largeurs de fenêtres, quel est le meilleur score que vous obtenez (avec la fonction `accuracy` de NLTK) sur l'ensemble de test ?  Comment se compare-t-il avec les précédents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy (train) with k=2: 0.9614368726888537\n",
      "Classifier accuracy (test) with k=2: 0.8945147679324894\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import naivebayes \n",
    "# Veuillez répondre ici à la question.\n",
    "\n",
    "\n",
    "# Dictionnaire pour stocker les résultats\n",
    "results = {}\n",
    "\n",
    "# En utilisant le trainset de base, voici les résultats :\n",
    "def classify_with_naive_bayes(tr_d, te_d, k=2):\n",
    "    \n",
    "    # Entraînement du classificateur Naive Bayes\n",
    "    c = naivebayes.NaiveBayesClassifier.train(tr_d)\n",
    "    \n",
    "    # Calcul de l'accuracy sur les ensembles d'entraînement et de test\n",
    "    train_acc = nltk.classify.accuracy(c, tr_d)\n",
    "    test_acc = nltk.classify.accuracy(c, te_d)\n",
    "\n",
    "    print(f\"Classifier accuracy (train) with k={k}:\", train_acc)\n",
    "    print(f\"Classifier accuracy (test) with k={k}:\", test_acc)\n",
    "    \n",
    "    return c, train_acc, test_acc\n",
    "\n",
    "# Classification avec Naive Bayes\n",
    "classifier, train_accuracy, test_accuracy = classify_with_naive_bayes(iwf_train, iwf_test)\n",
    "\n",
    "# Garde en mémoire le résultat pour la suite\n",
    "results['naive_bayes_k2'] = {\n",
    "    'train': train_accuracy,\n",
    "    'test': test_accuracy,\n",
    "    'window_size': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with k=1...\n",
      "Cleaned data, new length: 2367\n",
      "Classifier accuracy (train) with k=1: 0.9265715795034337\n",
      "Classifier accuracy (test) with k=1: 0.890295358649789\n",
      "\n",
      "Testing with k=3...\n",
      "Cleaned data, new length: 2367\n",
      "Classifier accuracy (train) with k=3: 0.9804543053354464\n",
      "Classifier accuracy (test) with k=3: 0.8924050632911392\n",
      "\n",
      "Testing with k=4...\n",
      "Cleaned data, new length: 2367\n",
      "Classifier accuracy (train) with k=4: 0.9846804014791336\n",
      "Classifier accuracy (test) with k=4: 0.8881856540084389\n",
      "\n",
      "Testing with k=5...\n",
      "Cleaned data, new length: 2367\n",
      "Classifier accuracy (train) with k=5: 0.9904912836767037\n",
      "Classifier accuracy (test) with k=5: 0.8860759493670886\n",
      "\n",
      "Testing with k=6...\n",
      "Cleaned data, new length: 2367\n",
      "Classifier accuracy (train) with k=6: 0.993660855784469\n",
      "Classifier accuracy (test) with k=6: 0.8734177215189873\n",
      "\n",
      "=== RÉSUMÉ DES RÉSULTATS ===\n",
      "naive_bayes_k2: Train=0.961, Test=0.895, Window=2\n",
      "naive_bayes_k1: Train=0.927, Test=0.890, Window=1\n",
      "naive_bayes_k3: Train=0.980, Test=0.892, Window=3\n",
      "naive_bayes_k4: Train=0.985, Test=0.888, Window=4\n",
      "naive_bayes_k5: Train=0.990, Test=0.886, Window=5\n",
      "naive_bayes_k6: Train=0.994, Test=0.873, Window=6\n",
      "\n",
      "Meilleur score: naive_bayes_k2 avec 0.895 sur le test\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 7):\n",
    "    if k == 2: # On a déjà testé k=2\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nTesting with k={k}...\")\n",
    "    \n",
    "    # Extraire les caractéristiques avec le nouveau k\n",
    "    iwf_train_k, iwf_test_k = split_data_train_test(extract_features(sentences, k), test_size=0.2)\n",
    "    \n",
    "    classifier_k, train_accuracy_k, test_accuracy_k = classify_with_naive_bayes(iwf_train_k, iwf_test_k, k)\n",
    "    \n",
    "    # Stocker dans le dictionnaire\n",
    "    results[f'naive_bayes_k{k}'] = {\n",
    "        'train': train_accuracy_k,\n",
    "        'test': test_accuracy_k,\n",
    "        'window_size': k\n",
    "    }\n",
    "\n",
    "# Afficher le résumé des résultats\n",
    "print(\"\\n=== RÉSUMÉ DES RÉSULTATS ===\")\n",
    "for method, scores in results.items():\n",
    "    print(f\"{method}: Train={scores['train']:.3f}, Test={scores['test']:.3f}, Window={scores['window_size']}\")\n",
    "\n",
    "# Trouver le meilleur score de test\n",
    "best_method = max(results.items(), key=lambda x: x[1]['test'])\n",
    "print(f\"\\nMeilleur score: {best_method[0]} avec {best_method[1]['test']:.3f} sur le test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification finale avec k=2...\n",
      "Cleaned data, new length: 2367\n",
      "Classifier accuracy (train) with k=2: 0.9614368726888537\n",
      "Classifier accuracy (test) with k=2: 0.8945147679324894\n"
     ]
    }
   ],
   "source": [
    "best_k = best_method[1]['window_size']\n",
    "print(f\"\\nClassification finale avec k={best_k}...\")\n",
    "iwf_train_best, iwf_test_best = split_data_train_test(extract_features(sentences, best_k), test_size=0.2)\n",
    "best_classifier, best_train_accuracy, best_test_accuracy = classify_with_naive_bayes(iwf_train_best, iwf_test_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4d.** En utilisant la fonction `show_most_informative_features()`, veuillez afficher les attributs les plus informatifs et commenter le résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   word0 = 'interests'         3 : 1      =     89.1 : 1.0\n",
      "                  word+1 = 'in'                5 : 6      =     87.6 : 1.0\n",
      "                  word+1 = 'of'                4 : 6      =     63.2 : 1.0\n",
      "                  word-1 = 'other'             3 : 6      =     42.3 : 1.0\n",
      "                  word+2 = '.'                 6 : 1      =     36.0 : 1.0\n",
      "                  word+2 = ','                 6 : 5      =     28.1 : 1.0\n",
      "                  word-2 = 'have'              1 : 6      =     19.5 : 1.0\n",
      "                  word-1 = 'in'                6 : 5      =     19.5 : 1.0\n",
      "                  word+2 = 'the'               5 : 2      =     18.1 : 1.0\n",
      "                  word-2 = 'company'           5 : 6      =     17.7 : 1.0\n",
      "                  word-1 = 'own'               4 : 6      =     17.4 : 1.0\n",
      "                  word-1 = 'and'               6 : 5      =     15.8 : 1.0\n",
      "                  word-2 = 'other'             3 : 6      =     15.3 : 1.0\n",
      "                  word-1 = '%'                 5 : 6      =     15.2 : 1.0\n",
      "                  word+1 = 'was'               1 : 6      =     14.6 : 1.0\n",
      "                  word+2 = 'and'               6 : 1      =     14.6 : 1.0\n",
      "                  word+2 = 'on'                6 : 5      =     12.1 : 1.0\n",
      "                  word+1 = 'because'           2 : 6      =     12.1 : 1.0\n",
      "                  word-1 = 'public'            4 : 1      =     11.9 : 1.0\n",
      "                  word-2 = 'the'               4 : 3      =     11.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des résultats\n",
    "\n",
    "## Informations des sens de *interest* :\n",
    "\n",
    "- Sense 1 =  361 occurrences (15%) - readiness to give attention\n",
    "- Sense 2 =   11 occurrences (01%) - quality of causing attention to be given to\n",
    "- Sense 3 =   66 occurrences (03%) - activity, etc. that one gives attention to\n",
    "- Sense 4 =  178 occurrences (08%) - advantage, advancement or favor\n",
    "- Sense 5 =  500 occurrences (21%) - a share in a company or business\n",
    "- Sense 6 = 1252 occurrences (53%) - money paid for the use of money\n",
    "\n",
    "## Analyse des caractéristiques les plus informatives :\n",
    "\n",
    "Dans les \"most informative features\", on compare le ratio de probabilité d'un mot entre un sens et un autre. Voici les observations principales :\n",
    "\n",
    "\n",
    "1. **`word0 = 'interests'` (89.1:1 pour sens 3 vs 1)** : Le pluriel \"interests\" indique clairement le sens 3 (activités). Ce qui fait assez sens, on dit \"my interests are ...\" pour parler d'activités/hobbies.\n",
    "\n",
    "2. **`word+1 = 'in'` (87.6:1 pour sens 5 vs 6)** : \"interest in\" favorise fortement le sens 5 (part d'entreprise). Ex: \"... interest in the company\".\n",
    "\n",
    "3. **`word+1 = 'of'` (63.2:1 pour sens 4 vs 6)** : \"interest of\" indique le sens 4 (avantage/intérêt personnel). Ex : \"in the interest of...\".\n",
    "\n",
    "4. **`word-1 = '%'` (15.2:1 pour sens 5 vs 6)** : Le symbole % avant \"interest\" favorise le sens 5 (part d'entreprise) plutôt que 6 (intérêt financier). Ex  : \"... 5% interest\".\n",
    "\n",
    "5. **`word-2 = 'company'` (17.7:1 pour sens 5 vs 6)** : \"company ... interest\" indique clairement le sens 5 (part d'entreprise).\n",
    "\n",
    "6. **Ponctuation** : Les points (.) et virgules (,) en position +2 favorisent différents sens, intéressant !\n",
    "\n",
    "7. **`word-1 = 'own'`** : \"own interest\" favorise le sens 4 (avantage personnel).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4e.** On souhaite également obtenir les scores pour chaque sens.  Pour ce faire, il faut demander les prédictions une par une au classifieur (voir le [livre NLTK](https://www.nltk.org/book/ch06.html)), et comptabiliser les prédictions correctes pour chaque sens.  Vous pouvez vous inspirer de `evaluate_wsd`, et écrire une fonction `evaluate_wsd_supervised(classifier, items_with_features)`, que vous appliquerez aux donnés `iwf_test`.  Veuillez afficher ces scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'word0': 'interest', 'word-1': 'short', 'word+1': 'dropped', 'word-2': 'that', 'word+2': 'to'}, '5'), ({'word0': 'interest', 'word-1': '}', 'word+1': 'rates', 'word-2': 'NONE', 'word+2': ','}, '6'), ({'word0': 'interests', 'word-1': 'minority', 'word+1': 'rose', 'word-2': 'to', 'word+2': 'to'}, '5')]\n"
     ]
    }
   ],
   "source": [
    "print(iwf_test_best[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCORES PAR SENS ===\n",
      "Sens 1: 55/72 = 76.4%\n",
      "Sens 2: 0/2 = 0.0%\n",
      "Sens 3: 11/13 = 84.6%\n",
      "Sens 4: 31/36 = 86.1%\n",
      "Sens 5: 86/100 = 86.0%\n",
      "Sens 6: 241/251 = 96.0%\n",
      "\n",
      "Score global: 424/474 = 89.5%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_wsd_supervised(classifier, items_with_features):\n",
    "\n",
    "    # Initialiser les compteurs pour chaque sens\n",
    "    sense_counts = {}  \n",
    "    sense_correct = {} \n",
    "    \n",
    "    total_correct = 0\n",
    "    total_instances = 0\n",
    "    \n",
    "    for features, true_sense in items_with_features:\n",
    "        # Extraire le numéro de sens (supprimer /NN, /NNS, etc.)\n",
    "        true_sense_num = true_sense\n",
    "        \n",
    "        # Prédiction du classifieur\n",
    "        predicted_sense = classifier.classify(features)\n",
    "        \n",
    "        # Rajoute le sens dans les compteurs s'il n'existe pas\n",
    "        if true_sense_num not in sense_counts:\n",
    "            sense_counts[true_sense_num] = 0\n",
    "            sense_correct[true_sense_num] = 0\n",
    "        \n",
    "        # Compter l'instance\n",
    "        sense_counts[true_sense_num] += 1\n",
    "        total_instances += 1\n",
    "        \n",
    "        # Vérifier si la prédiction est correcte\n",
    "        if predicted_sense == true_sense_num:\n",
    "            sense_correct[true_sense_num] += 1\n",
    "            total_correct += 1\n",
    "    \n",
    "    # Calculer les scores par sens\n",
    "    sense_scores = {}\n",
    "    for sense in sorted(sense_counts.keys()):\n",
    "        if sense_counts[sense] > 0:\n",
    "            accuracy = sense_correct[sense] / sense_counts[sense]\n",
    "            sense_scores[sense] = {\n",
    "                'correct': sense_correct[sense],\n",
    "                'total': sense_counts[sense],\n",
    "                'accuracy': accuracy\n",
    "            }\n",
    "    \n",
    "    # Score global pour être sûr d'avoir les mêmes valeurs que plus haut\n",
    "    global_accuracy = total_correct / (total_instances if total_instances > 0 else 0)\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    print(\"=== SCORES PAR SENS ===\")\n",
    "    for sense in sorted(sense_scores.keys()):\n",
    "        stats = sense_scores[sense]\n",
    "        print(f\"Sens {sense}: {stats['correct']}/{stats['total']} = {stats['accuracy']*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nScore global: {total_correct}/{total_instances} = {global_accuracy*100:.1f}%\")\n",
    "    \n",
    "    return sense_scores, global_accuracy\n",
    "\n",
    "# Appliquer la fonction aux données de test\n",
    "sense_scores, global_acc = evaluate_wsd_supervised(best_classifier, iwf_test_best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici qu'on a pas eu de chance pour le sens 2, mais que les autres sens sont bien prédits. Le sens 6 est très bien prédit, ce qui est logique car il est le plus fréquent et donc il doit être le plus facile à prédire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Veuillez recopier ci-dessous, en guise de conclusion, les scores des trois expériences réalisées, pour pouvoir les comparer d'un coup d'oeil.  Quel est le meilleur score obtenu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tableau récapitulatif des scores (Student's Notebook) :\n",
      "\n",
      "                  Méthode     Score (senses1)     Score (senses2)\n",
      "           Lesk simplifié 20.90% (fenêtre 16) 23.61% (fenêtre 18)\n",
      "                 word2Vec 41.43% (fenêtre 12) 62.42% (fenêtre 13)\n",
      "Classification supervisée              89.45%                 N/A\n",
      "\n",
      "Le meilleur score global obtenu dans ce notebook est de 89.45% avec la méthode de Classification supervisée.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "best_window_lesk_s1 = 16\n",
    "best_window_lesk_s2 = 18\n",
    "best_window_w2v_s1 = 12\n",
    "best_window_w2v_s2 = 13\n",
    "\n",
    "\n",
    "score_lesk_s1 = evaluate_wsd(wsd_lesk, senses1, sentences, window_size=best_window_lesk_s1)\n",
    "score_lesk_s2 = evaluate_wsd(wsd_lesk, senses2, sentences, window_size=best_window_lesk_s2)\n",
    "\n",
    "score_w2v_s1 = evaluate_wsd(wsd_word2vec, senses1, sentences, window_size=best_window_w2v_s1)\n",
    "score_w2v_s2 = evaluate_wsd(wsd_word2vec, senses2, sentences, window_size=best_window_w2v_s2)\n",
    "\n",
    "score_classification = nltk.classify.accuracy(classifier, iwf_test) * 100\n",
    "\n",
    "summary_data = {\n",
    "    \"Méthode\": [\"Lesk simplifié\", \"word2Vec\", \"Classification supervisée\"],\n",
    "    \"Score (senses1)\": [\n",
    "        f\"{score_lesk_s1:.2f}% (fenêtre {best_window_lesk_s1})\",\n",
    "        f\"{score_w2v_s1:.2f}% (fenêtre {best_window_w2v_s1})\",\n",
    "        f\"{score_classification:.2f}%\"\n",
    "    ],\n",
    "    \"Score (senses2)\": [\n",
    "        f\"{score_lesk_s2:.2f}% (fenêtre {best_window_lesk_s2})\",\n",
    "        f\"{score_w2v_s2:.2f}% (fenêtre {best_window_w2v_s2})\",\n",
    "        \"N/A\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_conclusion_student = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"Tableau récapitulatif des scores (Student's Notebook) :\\n\")\n",
    "print(df_conclusion_student.to_string(index=False)) # .to_string() pour un meilleur affichage sans index\n",
    "\n",
    "# Commentaire sur le meilleur score\n",
    "meilleur_score_global = score_classification # Basé sur les chiffres\n",
    "print(f\"\\nLe meilleur score global obtenu dans ce notebook est de {meilleur_score_global:.2f}% avec la méthode de Classification supervisée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du laboratoire\n",
    "\n",
    "Merci de nettoyer votre feuille, sauvegarder le résultat, et soumettre le *notebook* sur Cyberlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CoursTAL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
